---
title: "Étude de cas - GLM et valeurs manquantes"
author: "Romane Lacoste-Badie, Margaux Touzé et Camille Loisel"
date: "02/03/2022"
header-includes:
  - \DeclareUnicodeCharacter{0301}{/}
  - \usepackage{float}
output:
  bookdown::pdf_document2: default
  number_section : true
  extra_dependecies : ["float"]
bibliography: mes_ref_biblio.bib
---

```{=tex}
\clearpage
\renewcommand{\contentsname}
\tableofcontents
```
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6.5, fig.height = 3.5, fig.pos="H", out.extra = "")
library(lmtest)
library(stats)
library(car)
library(MASS)
library(ResourceSelection)
library(ROCR)
library(dplyr)
library(knitr)
library(gridExtra)
library(ggplot2)
library(parallel)
library(stats)
library(mvtnorm)
library(visdat)
library(naniar)
library(UpSetR)
library(bookdown)
library(mice)
library(VIM)
library(lattice)
```

\clearpage

# Modèles linéaires généralisés (GLM) avec R

```{r, echo=F}
mites <- read.csv("mites.csv")
```

## Introduction

L'objectif de cette première partie est d'étudier le jeu de données sur les mites Oribatid mis à disposition par P. Legendre et D. Borcard et décrit dans leur article [@Borcard1994]. Tout particulièrement, nous essaierons de construire différents modèles linéaires simples puis généralisés pour décrire trois des variables de ce jeu de données.

Pour leur étude, Legendre et Borcard ont choisi une zone située à Saint-Hippolyte, Canada, qu'ils ont divisée en 70 "coeurs". Pour chaque coeur, ils ont récupéré des données environnementales et sur la faune. Le jeu de données que l'on étudie est donc constitué de 70 observations qui correspondent aux coeurs, et de 9 variables décrites ci-dessous.

```{r}
str(mites)
```

-   **Galumna** (entier) : nombre de mites *Galumna sp*.

-   **pa** (0 ou 1) : présence (1) ou absence (0) de mites *Galumna sp*.

-   **totalabund** (entier) : nombre total de mites

-   **prop** (réel entre 0 et 1) : proportion de mites *Galumna sp*.

-   **SubsDens** (réel) : densité du substrat en $g.L^{-1}$ de matière sèche non comprimée

-   **WatrCont** (réel) : contenu en eau du substrat en $g.L^{-1}$

-   **Substrate** (facteur) : type de substrat, facteur à 7 modalités

-   **Shrub** (facteur) : buissons, facteur à trois modalités

-   **Topo** (facteur) : microtopographie, facteur à 2 modalités

```{r, echo=F}
mites$pa <- as.factor(mites$pa)
mites$Substrate <- as.factor(mites$Substrate)
mites$Shrub <- as.factor(mites$Shrub)
mites$Topo <- as.factor(mites$Topo)
```

Nous allons maintenant représenter ces données afin de mieux les appréhender.\

\newpage

Commençons par les variables que l'on va chercher à expliquer par la suite : `Galumna`, `pa` et `prop.`

```{r, echo=F, fig.cap="Distribution de Galumna"}
ggplot(data = mites) + geom_bar(mapping = aes(Galumna), fill='darkred') +labs(y = "Nombre de coeurs", x = "Nombre de Galumna")
```

On remarque que plus de la moitié des coeurs étudiés ne contiennent pas de mites Galumna. La valeur maximale de Galumna contenues dans un coeur est 8. \newline

```{r, echo=F, fig.cap="Distribution de pa"}
ggplot(data = mites) + geom_bar(mapping = aes(pa), fill='darkred') +labs(y = "Nombre de coeurs", x = "Absence ou Présence de Galumna")
```

On voit ci-dessus plus précisément que 45 coeurs ne contiennent pas de Galumna et 25 coeurs en contiennent. \newline

```{r, echo=F, fig.cap="Distribution de prop"}
ggplot(data=mites) + geom_histogram(aes(prop), fill="darkred", bins = 30) +labs(y = "Nombre de coeurs", x = "Proportion de Galumna")
```

Naturellement, on remarque que 45 coeurs ont une proportion de Galumna de 0%. Pour les autres coeurs, la proportion varie entre 0 et 0.06%. \newline

Passons maintenant aux variables explicatives. \newline

```{r, echo=F, fig.cap="Distribution de SubsDens"}
ggplot(data=mites) + geom_histogram(aes(SubsDens), fill="darkred", binwidth = 5) +labs(y = "Nombre de coeurs", x = "SubsDens")
```

```{r, echo=F, fig.cap="Distribution de WatrCont"}
ggplot(data=mites) + geom_histogram(aes(WatrCont), fill="darkred", binwidth = 40) +labs(y = "Nombre de coeurs", x = "Watrcont")
```

Nous pouvons observer la distribution des variables quantitatives SubsDens et WatrCont. Cependant, ces variables étant déterministes, on ne fait pas d'hypothèse de normalité. Donc nous n'avons pas besoin de "vérifier" qu'elles suivent une loi normale, ni d'effectuer des transformations pour que cela soit le cas. \newline

```{r, echo=F, fig.cap="Distribution de Substrate"}
ggplot(data = mites) + geom_bar(mapping = aes(Substrate), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Substrate")
```

Les deux types de substrat majoritaires sont "Interface" et "Sphagn1", qui sont chacun le substrat d'environ 25 coeurs sur les 70 totaux. \newline

```{r, echo=F, fig.cap="Distribution de Shrub"}
ggplot(data = mites) + geom_bar(mapping = aes(Shrub), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Shrub")
```

La variable `Shrub` représente les buissons du coeur. On voit qu'un peu moins d'une vingtaine de coeurs ne contient pas de buisson. Puis environ 25 coeurs contiennent beaucoup de buissons et peu de buissons. \newline

```{r, echo=F, fig.cap="Distribution de Topo"}
ggplot(data = mites) + geom_bar(mapping = aes(Topo), fill="darkred")  +labs(y = "Nombre de coeurs", x = "Topo")
```

\newpage

Finalement, on voit qu'environ 45 coeurs ont une topographie de type "Blanket", les autres ont une topographie de type "Hummock".\
Ce premier coup d'oeil aux données nous a permis de remarquer quelque chose d'important : plus de la moitié des coeurs ne contiennent pas de Galumna. Cela siginifie, pour ceux-ci, que la proportion de Galumna est nulle, que la variable Galumna est égale à 0 et que la variable occurence de Galumna est également égale à 0. Il est important de garder cela à l'esprit pour la future construction de nos modèles.

\clearpage

## LM Gaussien

Dans un premier temps, nous allons essayer d'ajuster un modèle linéaire gaussien pour chaque variable réponse que l'on cherche à expliquer : `Galumna`, `prop`, `pa`. Mais un tel modèle doit vérifier plusieurs hypothèses. Le premier travail consiste donc à regarder si ces hypothèses sont vérifiées.

### Vérification des hypothèses

Rappelons les hypothèses du modèle linéaire gaussien :

-   sur la forme du modèle :\
    (H1) **linéarité** du modèle\

-   sur les erreurs $\epsilon_i$ qui sont :\
    (H2) **centrées** : $\mathbb{E}(\epsilon_i)=0, \forall i=1,..n$, cette condition est toujours vérifiée par les moindres carrés ordinaires (MCO), technique de résolution de la régression linéaire\
    (H3) **homoscédastiques** : $\mathbb{V}(\epsilon_i)=\sigma^2, \forall i$\
    (H4) **non-autocorrélées** : $cor(\epsilon_i, \epsilon_{i}^{'})=0, \forall i\neq i'$\
    (H5) **normales** : $\epsilon_i \sim N(0, \sigma^2)$\

-   sur les variables explicatives $X_1,...,X_p$ qui sont :\
    (H6) **non aléatoires** : la théorie se généralise facilement pour les variables aléatoires\
    (H7) **non multicolinéaires** : les variables $X_1,...,X_p$ sont linéairement indépendantes, ce qui garantit l'unicité de l'estimateur MCO.

Pour chaque modèle expliquant `Galumna`, `prop` et `pa`, nous regarderons si toutes ces hypothèses sont vérifiées, sauf la **2** et la **6** qui sont toujours vérifiées.

```{r, echo=F}
mites$pa <- as.numeric(as.character(mites$pa))
```

-   **(H1) Linéarité du modèle**

Nous allons observer la linéarité du modèle dans un premier temps par représentation graphique. Nous représenterons chaque variable à expliquer en fonction des variables `WatrCont` et `SubsDens` qui sont les deux seules variables quantitatives explicatives.

\newpage

**Variable réponse Galumna :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse Galumna"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=Galumna)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(Galumna~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(Galumna~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=Galumna)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(Galumna~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(Galumna~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On ne voit pas un lien linéaire évident entre `Galumna` et `SubsDens`, ni entre `Galumna` et `WatrCont.` On voit que la quantité importante de points sur l'axe des abscisses (pour lesquels `Galumna`=0) semble beaucoup impacter la droite de régression.

**Variable réponse pa :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse pa"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=pa)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(pa~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(pa~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=pa)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(pa~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(pa~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On ne voit pas non plus un lien linéaire entre la variable `pa` et les variables explicatives. De plus, comme c'est une variable binaire, il semblerait plus naturel de penser à une régression logistique qu'à une régression linéaire.

**Variable réponse prop :**

```{r, echo=F, fig.cap="Modèles de régression sur la variable réponse prop"}
p1 <- ggplot(data=mites, aes(x=SubsDens, y=prop)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(prop~SubsDens)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(prop~SubsDens)"), values=c("darkred")) + theme(legend.position = "top")
p2 <- ggplot(data=mites, aes(x=WatrCont, y=prop)) + geom_point() + geom_smooth(method="lm", formula = y~x, aes(color="lm(prop~WatrCont)"), se=F) +scale_colour_manual(name="Regression model", breaks=c("lm(prop~WatrCont)"), values=c("darkred")) + theme(legend.position = "top")
grid.arrange(p1,p2, nrow=1)
```

On fait le même constat pour la variable `prop`, il ne semble pas y avoir un lien linéaire particulier et la présence de beaucoup de points sur l'axe des abscisses semble influencer fortement la droite de régression.

Pour pousser notre analyse plus loin, nous allons effectuer un test de Harvey-Collier sur les modèles qui expliquent nos variables réponses par les deux variables explicatives quantitatives (`SubsDens` et `WatrCont`). Le test de Harvey-Collier (fonction `lmtest:harvtest`) consiste à faire un test de Student sur les résidus récursifs. Si la réelle relation entre les variables n'est pas linéaire mais convexe ou concave, la moyenne des résidus récursifs devrait être significativement différente de 0.

```{r, echo=F}
kable(data.frame(c(harvtest(lm(Galumna~WatrCont+SubsDens, data=mites))$p.value,
                     harvtest(lm(pa~WatrCont+SubsDens, data=mites))$p.value,
                     harvtest(lm(prop~WatrCont+SubsDens, data=mites))$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
                  col.names = "p-value Harvey-Collier Test",
      caption="Harvey-Collier Test", position="H")
```

Au seuil de 5%, on peut conclure que :

-   la relation entre la variable `pa` et les variables `WatrCont` et `SubsDens` n'est pas linéaire

-   la relation entre la variable `Galumna` et les variables `WatrCont` et `SubsDens` est linéaire

-   la relation entre la variable `prop` et les variables `WatrCont` et `SubsDens` est linéaire

À ce stade de l'analyse, on peut déjà conclure qu'il ne sera pas possible d'ajuster un modèle linéaire sur le variable `prop` car l'hypothèse de linéarité n'est pas vérifiée.

-   **(H3) Erreurs** $\epsilon_i$ homoscédastiques

Pour la suite de la vérification des hypothèses, on ajuste un modèle complet sur chacune de nos variables réponses (i.e. un modèle qui fait intervenir toutes les variables explicatives à notre disposition).

```{r, echo=F}
lm.tot.g<-lm(Galumna~.-pa-prop-totalabund, data=mites)
lm.tot.pa<-lm(pa~.-Galumna-prop-totalabund, data=mites)
lm.tot.prop<-lm(prop~.-pa-Galumna-totalabund, data=mites)
```

```{r, echo=F, warning=F, fig.height=5, fig.cap="Variances des résidus des modèles de régression linéaire"}
par(mfrow=c(2,2))
plot(lm.tot.g, which = 3)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.pa, which = 3)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.prop, which = 3)
title(main="lm(prop~.)", line=1.4)
```

Le type de graphique représenté ci-dessus est utilisé pour évaluer l'homoscédasticité des résidus. Si leur variance est bein constante, alors la courbe rouge doit former une ligne à peu près horizontale. On voit que cette hypothèse est plutôt bien respectée pour les modèles expliquant les variables `Galumna` et `prop.` Par contre, l'hypothèse n'est manifestement pas vérifiée pour le modèle expliquant la variable `pa.`

La vérification de cette hypothèse peut également se faire par un test de Breusch-Pagan (fonction `lmtest:bptest`) pour lequel on a $H_0$ : le bruit est homoscédastique, i.e. $\sigma_i^2=\sigma^2$ pour tout $i$.

```{r, echo=F}
kable(data.frame(c(bptest(lm.tot.g)$p.value,
                     bptest(lm.tot.pa)$p.value,
                     bptest(lm.tot.prop)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
                    col.names = "p-value Bruesch-Pagan Test",
                    caption="Breusch-Pagan Test", position="H")
```

Ce test confirme la conclusion basée sur les graphiques : on rejette l'hypothèse d'homoscédasticité des résidus pour le modèle expliquant `pa`, mais on l'accepte pour les deux autres modèles.

-   **(H4) Erreurs** $\epsilon_i$ non corrélées

Afin de tester la non-autocorrélation des résidus du modèle linéaire, nous allons effectuer un test de Durbin-Watson (fonction `lmtest:dwtest`). Ce test cherche à évaluer la significativité du coefficient $\rho$ dans la formule : $\epsilon_t = \rho \epsilon_{t-1} + u_t$ où $\epsilon_t$ est le résidu estimé du modèle et $u_t$ un bruit blanc avec le test de Wald. L'hypothèse nulle est donc $H_0 : \rho=0$, il y a non-autocorrélation des résidus.

```{r, echo=F}
kable(data.frame(c(dwtest(lm.tot.g)$p.value,
                     dwtest(lm.tot.pa)$p.value,
                     dwtest(lm.tot.prop)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
      col.names = "p-value Durbin-Watson Test",
      caption="Durbin-Watson Test", position="H")
```

On observe une p-value inférieure à 0.05 pour les modèles expliquant `Galumna` et `prop.` On peut dire qu'au seuil de 5%, on considère que l'hypothèse de non-autocorrélation des erreurs n'est pas respectée pour ces deux modèles. Elle est cependant vérifiée pour le modèle linéaire expliquant `pa.`\
À ce stade de l'analyse, nous avons constaté que les hypothèses 1 et 3 ne sont pas vérifiées pour le modèle linéaire expliquant `pa`. L'hypothèse 4 n'est quant à elle pas vérifiée pour les modèles linéaires expliquant `Galumna` et `prop.` On pourrait donc s'arrêter et dire qu'il n'est pas possible d'ajuster un modèle lin"aire sur ces trois variables. Par curiosité, nous allons continuer à regarder si les hypothèses suivantes sont vérifiées, bien que l'issue concernant les modèles linéaires reste la même.

-   **(H5) Normalité des erreurs** $\epsilon_i$

Dans le cadre d'un modèle linéaire, on suppose $\epsilon_i \sim N(0, \sigma^2)$. Une façon de vérifier cette hypothèse est de tracer les QQ-plot de nos modèles linéaires.

```{r, echo=F, warning =F, fig.height=5, fig.cap="QQ-plot des modèles de régression linéaire"}
par(mfrow=c(2,2))
plot(lm.tot.g, which = 2)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.pa, which = 2)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
```

On voit que les points de nos différents modèles suivent à peu près tous la droite de normalité. Cependant, on remarque de nombreux points déviant de cette droite. Il est difficile de tirer une conclusion à partir de ces graphiques seuls. Il est également possible de réaliser un test de Shapiro-Wilk (fonction `stats:shapiro.test`). Il teste l'hypothèse nulle selon laquelle un échantillon $x_1,...,x_n$ est issu d'une population normalement distribuée. Ici, notre échantillon sera les $\epsilon_1,...,\epsilon_n$ du modèle linéaire considéré.

```{r, echo=F}
kable(data.frame(c(shapiro.test(lm.tot.g$residuals)$p.value,
                     shapiro.test(lm.tot.pa$residuals)$p.value,
                     shapiro.test(lm.tot.prop$residuals)$p.value),
                    row.names=c("Galumna~WatrCont+SubsDens",
                                  "pa~WatrCont+SubsDens",
                                  "prop~WatrCont+SubsDens")),
      col.names = "p-value Shapiro-Wilk Test",
      caption="Shapiro-Wilk Test", position="H")
```

On observe une très petite p-value pour les trois modèles linéaires. On considère que l'hypothèse de normalité des résidus n'est pas vérifiée pour nos trois modèles.

-   **(H7) Non-multicolinéarité des variables explicatives**

La dernière hypothèse qui doit être vérifiée lorsque l'on ajuste un modèle linéaire est la non-multicolinéarité des variables explicatives. Pour la vérifier, on peut faire de l'analyse bivariée, c'est-à-dire vérifier si les variables semblent être corrélées deux à deux, ou encore calculer les VIF.

**Corrélations SubsDens\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable SubsDens"}
p1 <- ggplot(data=mites, aes(x = WatrCont, y=SubsDens)) + geom_point(color="darkred") + labs(title = "SubsDens ~ WatrCont") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p2 <- ggplot(data=mites, aes(Substrate, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Substrate") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold")) 

p3 <- ggplot(data=mites, aes(Shrub, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Shrub") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p4 <- ggplot(data=mites, aes(Topo, SubsDens)) + geom_boxplot(color="darkred") + labs(title = "SubsDens ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
grid.arrange(p1, p2, p3, p4, nrow=2)
```

Il ne semble pas y avoir de corrélation particulière entre les variables quantitatives `WatrCont` et `SubsDens`, les points sont répartis de manière assez aléatoire. Il ne semble pas y avoir de corrélation entre `SubsDens` et les variables `Shrub` et `Topo` non plus : la moyenne de `SubsDens` ne semble pas varier significativement selon le facteur `Shrub` ou `Topo.` Cependant, le type de substrat (variable `Substrate`) semble influencer significativement la densité du substrat (`SubsDens`), cela paraît assez logique.

\newpage

**Corrélations WatrCont\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable WatrCont"}
p1 <- ggplot(data=mites, aes(Substrate, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Substrate") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p2 <- ggplot(data=mites, aes(Shrub, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Shrub") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))

p3 <- ggplot(data=mites, aes(Topo, WatrCont)) + geom_boxplot(color="darkred") + labs(title = "WatrCont ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
grid.arrange(p1, p2, p3, nrow=2)
```

Les variables `Substrate` et `Shrub` semblent avoir une influence significative sur la quantité d'eau (variable `WatrCont`). Cependant, il est difficile de conclure qu'il y a présence ou absence de corrélation entre `WatrCont` et `Topo.`

\newpage

**Corrélations Substrate\~ :**

```{r, echo=F, fig.height=5, fig.cap="Corrélations avec la variable Substrate"}
p1 <- ggplot(data=mites, aes(Substrate, fill=Shrub)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000", "#CC0000")) + labs(title = "Substrate ~ Shrub") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
p2 <- ggplot(data=mites, aes(Substrate, fill=Topo)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000", "#CC0000")) + labs(title = "Substrate ~ Topo") + scale_x_discrete(labels = c('B','I','L', 'Sph1','Sph2','Sph3','Sph4')) + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
grid.arrange(p1, p2, nrow=1)
```

Du fait du peu de donées que nous avons à disposition, il est difficile de conclure à partir des graphiques.

\newpage

**Corrélations Shrub\~ :**

```{r, echo=F, fig.cap="Corrélations avec la variable Shrub"}
p1 <- ggplot(data=mites, aes(Shrub, fill=Topo)) + geom_bar() +scale_fill_manual(values=c("#330000", "#990000")) + labs(title = "Shrub ~ Topo") + theme(plot.title = element_text(size = 10, color = "darkred", face = "bold"))
p1
```

Une nouvelle fois, au vu du faible nombre d'observations, il serait risqué de conclure uniquement à partir du graphique ci-dessus.

**Calcul des VIF :**

```{r, echo=F}
kable(data.frame(vif(lm.tot.g)[,1],vif(lm.tot.pa)[,1],vif(lm.tot.prop)[,1],
                    row.names=c("SubsDens", "WatrCont", "Substrate", "Shrub", "Topo")),
      col.names = c("lm(Galumna~.)","lm(pa~.)","lm(prop~.)"),
      caption="VIF", position="H")
```

Les VIF sont par définition toujours supérieurs à 1. On considère que le VIF d'une variable devient trop élevé lorsque sa valeur dépasse 5. Ce n'est pas le cas pour notre modèle. On peut donc conclure grâce aux VIF, et à notre analyse bivariée qu'il y a non-multicolinéarité entre les variables explicatives.

**Conclusion :** après analyse de toutes les hypothèses, on conclut qu'au moins une des hypothèses n'est pas vérifiée pour chacun de nos modèles linéaires. Il est donc impossible d'ajuster un modèle linéaire sur nos données pour expliquer les variables `Galumna`, `pa` et `prop.` Cependant, on peut essayer d'effectuer des transformations sur les données afin qu'elles vérifient les hypothèses.

\newpage

### Transformations sur les données

Transformer les données peut parfois régler les problèmes posés par les hypothèses du modèle linéaire gaussien, et notamment la non-normalité et l'hétéroscédasticité des résidus. Cependant, ces transformations présentent des inconvénients :

-   elles changent la variable réponse, ce qui peut compliquer l'interprétation

-   elles ne peuvent pas toujours améliorer la linéarité et l'homogénéité de la variance en même temps

-   les bornes de l'espace d'échantillonnage changent

Dans le cadre de notre étude, nous essaierons des transformations adaptées aux données en proportion pour notre variable `prop.` Ensuite, nous essaierons une transformation plus générale : celle de Box & Cox.

#### Transformation pour les données en proportion

 

Deux des principales transformations qui permettent de stabiliser la variance pour les données en proportion sont la transformation $logit$ et la transformation $arcsin$. Cependant, pour des données écologiques, la transformation $arcsin$ est souvent préférée car il est courant d'avoir des proportions à 0% ou 100%, ce qui pose problème pour effectuer une transformation $logit$. En effet, cela ferait donnerait lieu à des valeurs de $-\infty$ pour des proportions égales à 0. On rappelle que c'est notre cas : nous avions remarqué, lors de notre analyse préliminaire des données, que plusieurs observations avaient une valeur de 0 pour la variable prop. Il est donc impossible ici d'utiliser une transformation $logit$ sur les données.

**Transformation Arcsin :**

```{r, echo=F, fig.cap="Transformation Arcsin"}
asinTransform <- function(p) { asin(sqrt(p)) }
prop.asin <- asinTransform(mites$prop)
ggplot(data=data.frame(mites$prop, prop.asin),aes(x=mites$prop, y=prop.asin)) + geom_point() + geom_line(color="darkred")+ labs(x = "prop", y="Arcsin(prop)")
```

On voit ci-dessus que la transformation $arcsin$ semble finalement assez proche d'une transformation linéaire. Regardons ce que cela provoque sur les hypothèses du modèle linéaire gaussien.\
Rappelons que nous avions conclu, avant transformation, que l'hypothèse d'homoscédasticité des résidus était vérifiée. Cependant, ce n'était pas le cas pour l'hypothèse de normalité des résidus. Regardons ce qu'il en est après transformation.

```{r, echo=F, warning=F, fig.cap="QQ-plot Transformation Arcsin"}
lm.prop.asin <- lm(prop.asin~.-pa-prop-totalabund-Galumna, data =mites)
par(mfrow=c(1,2))
plot(x=lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
plot(lm.prop.asin, which =2)
title(main="lm(Arcsin(prop)~.)", line=1.4)
```

Graphiquement, la transformation $arcsin$ ne semble pas avoir amélioré la normalité des résidus. Vérifions avec un test de Shapiro-Wilk :

```{r, echo=F}
cat("p-value Shapiro-Wilk Test :",shapiro.test(lm.prop.asin$residuals)$p.value)
```

Une nouvelle fois, on rejette $H_0$ et on conclut que l'hypothèse de normalité des résidus n'est toujours pas vérifiée, même après une transformation $arcsin$.

#### Transformation de Box-Cox

 

La tranformation de Box & Cox est une transformation non linéaire souvent utilisée pour rendre les données normales. L'objectif est donc d'obtenir une distribution normale des données après transformation, et une variance constante. La transformation s'obtient comme suit : $$
\{ y \in \mathbb{R}^+_*, \lambda \in \mathbb{R} \} : y^*=f(y, \lambda)= 
\left\{
\begin{array}{ll}
\frac{y^{\lambda} - 1}{\lambda} \hspace{2em} (\lambda \neq 0) \\
\log(y) \hspace{2em} (\lambda = 0)
\end{array}
\right.
$$Dans le cas où $\lambda=0$, on retrouve une transformation logarithmique classique. Dans le cas où $\lambda=1$, cela revient à ne pas faire de transformation et de conserver notre variable d'origine à une translation près. Pour un échantillon de $n$ observations, on applique cette même transformation à chaque valeur. Attention, la transformation n'est pas définie pour les valeurs négatives ou nulles de la variable. Dans ce cas de figure, Box & Cox proposent de translater toutes les valeurs :

$$
\{ y \in \mathbb{R}^+_*, \lambda_1 \in \mathbb{R}, \lambda_2 \in \mathbb{R} \} : y^*=f(y, \lambda_1, \lambda_2)= 
\left\{
\begin{array}{ll}
\frac{(y+\lambda_2)^{\lambda_1} - 1}{\lambda} \hspace{2em} (\lambda_1 \neq 0) \\
\log(y+\lambda_2) \hspace{2em} (\lambda_1 = 0)
\end{array}
\right.
$$Il faut choisir $\lambda_2$ tel que $\forall y, y+\lambda_2>0$. Dans notre cas, on fixera $\lambda_2=1$ et on utilisera la fonction `MASS::boxcox` pour estimer $\lambda_1$. Appliquons maintenant cette transformation à chacune de nos variables réponses, puis re-vérifions les hypothèses du modèle linéaire gaussien, et plus particulièrement l'hypothèse de normalité qui était rejetée pour nos trois modèles et qui est supposée avoir été améliorée par la transformation.

**Variable répnose Galumna :**

```{r,echo=F}
lm.tot.g.translat <- lm((Galumna+1)~.-pa-prop-totalabund, data=mites)
bc <- boxcox(lm.tot.g.translat, lambda=seq(-3,2,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

Si la valeur $0$ avait été contenue par l'intervalle de confiance pour $\lambda_1$, il aurait été judicieux de simplement effectuer une transformation logarithmique. Ici, ce n'est pas le cas, on va donc réajustes un modèle sur les données transformées par Box & Cox avec le $\lambda_1$ qui maximise la log-vraisemblance (ici $\lambda_1=-1.8$).

```{r, echo=F, warning=F}
lm.tot.g.bc <- lm(((Galumna+1)^lambda-1)/lambda ~.-pa-prop-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.g, which = 2)
title(main="lm(Galumna~.)", line=1.4)
plot(lm.tot.g.bc, which = 2)
title(main="lm(BoxCox(Galumna)~.)", line=1.4)
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.g.bc$residuals)$p.value)
```

Graphiquement, la transformation de Box & Cox ne semble pas avoir amélioré la normalité des résidus. Pour le test de Shapiro-Wilk, on obtient une p-value inférieure à 5% donc on rejette l'hypothèse de normalité des résidus. La transformation de Box & Cox pour la variable Galumna n'a pas réglé le problème des hypothèses du modèle linéaire gaussien non vérifiées.

\newpage

**Variable réponse pa :**

```{r,echo=F}
lm.tot.pa.translat <- lm((pa+1)~.-Galumna-prop-totalabund, data=mites)
bc <- boxcox(lm.tot.pa.translat, lambda=seq(-5,2,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

On réajuste un modèle sur les données transformées par Box & Cox avec $\lambda_1=-4.5$.

```{r, echo=F, warning=F}
lm.tot.pa.bc <- lm(((Galumna+1)^lambda-1)/lambda ~.-pa-prop-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.pa, which = 2)
title(main="lm(pa~.)", line=1.4)
plot(lm.tot.pa.bc, which = 2)
title(main="lm(BoxCox(pa)~.)", line=1.4)
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.pa.bc$residuals)$p.value)
```

Une nouvelle fois, graphiquement la transformation ne semble pas avoir amélioré la normalité des résidus. Pour le test de Shapiro-Wilk, on obtient une p-value un tout petit peu supérieure à 5%. On considère que la p-value n'est pas assez élevée pour conclure à une normalité des résidus.

\newpage

**Variable réponse prop :**

```{r,echo=F}
lm.tot.prop.translat <- lm((prop+1)~.-pa-Galumna-totalabund, data=mites)
bc <- boxcox(lm.tot.prop.translat, lambda=seq(-200,0,0.1), plotit = T)
lambda <- bc$x[which.max(bc$y)]
cat("Lambda_1 optimal : ", lambda)
```

On réajuste un modèle sur les données transformées par Box & Cox avec $\lambda_1=-147$. Il s'agit d'une valeur de $\lambda_1$ très élevée comparée aux valeurs précédents, mais c'est celle qui maximise la log-vraisemblance.

```{r, echo=F, warning=F}
lm.tot.prop.bc <- lm(((prop+1)^lambda-1)/lambda ~.-pa-Galumna-totalabund, data=mites)
par(mfrow=c(1,2))
plot(lm.tot.prop, which = 2)
title(main="lm(prop~.)", line=1.4)
plot(lm.tot.prop.bc, which = 2)
title(main="lm(BoxCox(prop)~.)", line=1.4)
cat("p-value pour le test de Shapiro-Wilk : ",shapiro.test(lm.tot.g.bc$residuals)$p.value)
```

Graphiquement, la transformation semble avoir légèrement amélioré la normalité des résidus (ce n'est pas la même échelle pour $y$ pour les deux graphiques). Cependant, d'après le test de Shapiro-Wilk, on rejette l'hypothèse de normalité. \newpage \*\*Conclusion :\*\*

Dans cette première partie de l'analyse du jeu de données "mites", nous avons tenté d'ajuster un modèle linéaire gaussien sur trois variables explicatives. Cependant, l'une des difficultés lorsque l'on ajuste un modèle linéaire est qu'il doit vérifier un certain nombre d'hypothèses. Dans notre cas, il y avait toujours au moins une hypothèses qui n'était pas vérifiée. Nous avons donc mis en place des transformations de données, adaptées à nos variables, afin que le modèle vérifie les hypothèses. Malheureusement, ces transformations n'ont pas permis de valider les hypothèses, en particulier celle de normalité des résidus.\
Dans la pratique, il existe de nombreuses autres distributions de probabilité pour décrire les données : Poisson, Bernouilli, Binomiale... Nous allons donc essayer une autre approche pour ajuster un modèle sur nos données : les modèles linéaires généralisés.\
Un modèle linéaire généralisé a quatre composantes :

1.  les variables réponses $Y_1, Y_2,…, Y_n$ sont supposées suivre la même distrubtion appartenant à la **famille exponentielle**

2.  un vecteur de paramètres $\beta$ à estimer et des variables explicatives $X=[1,x^ 1,…,x^p]$

3.  une fonction de lien $g$ telle que\
    $g(\mu_i) = x_i^T\beta$ avec $E(Y_i)=\mu_i$

4.  une fonction de variance $V(\mu_i)$ qui décrit comment la variance $Var(Y_i)$ dépend de l'espérance $E(Y_i)=\mu_i$: $$Var(Y_i)=\phi V(\mu_i)$$ où $\phi$ est un paramètre de dispersion ou facteur d'échelle (constante) et $V$ une fonction de variance.\
    Finalement, le modèle linéaire gaussien est un cas particulier du modèle linéaire généralisé où l'on a $Y=X\beta + \epsilon$ avec les $\epsilon_i$ i.i.d. suivant $N(0,\sigma^2)$. Dans ce cas, $\mu_i=E(Y_i)=x_i^T\beta$ et la fonction de lien $g$ est la fonction identité $g(\mu_i)=\mu_i=x_i^T\beta$.

Sous R, la fonction utilisée pour les modèles linéaires généralisés est `glm`.

Nous allons donc essayer des modèles linéaires généralisés avec différentes distributions et fonctions de lien pour nos données :

-   une régression logistique binaire sur la variable `pa`

-   une régression logistique sur la variable `prop`

-   une régression de Poisson sur la variable `Galumna`

\newpage

## GLM Régression Logistique binaire sur la variable d'occurrence (pa)

**Objectif** : expliquer l'occurence `pa`, déterminer le meilleur modèle, interpréter les coefficients, puis évaluer l'ajustement de ce modèle et son pouvoir prédictif.

La variable binaire est un type de variable commun dans les données en écologie ou en santé : on observe un phénomène $Y$ ou son absence. Dans notre cas, on veut savoir si l'occurence de mites Galumna varie en fonction de l'environnement.

Ici, le modèle linéaire généralisé est composé de :

1.  la variable réponse $Y_i, i=1,...,n$ qui suit une distribution de Bernouilli de paramètre $\pi_i$

2.  un vecteur de paramètre $\beta$ à estimer et des variables explictives $X=[1,x^1,…,x^p]$

3.  la fonction de lien $g$ nommée $logit$. Elle est définie de $[0,1]$ sur $\mathbb{R}$ par $g(\pi)=ln(\frac{\pi}{1-\pi})$, soit $\pi_i=g^{-1}(x_i^T\beta)=\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)}$

Ainsi, on suppose ici que $y_i$ suit une loi de Bernouilli $B(1,\frac{\exp(x_i^T\beta)}{1+\exp(x_i^T\beta)})$.

Sous R, on indique en paramètre de la fonction `family="binomial"` et il n'est pas nécessaire de préciser la fonction de lien car par défaut, pour la famille binomiale, la fonction de lien est $logit$.

### Recherche du meilleur modèle

```{r, echo=F}
mites$pa <- as.factor(mites$pa)
```

-   **Méthode "à la main"**

Pour débuter, on construit le modèle complet (avec toutes les variables explicatives) pour déterminer lesquelles sont significatives :

```{r, echo=F}
glm.tot.pa <- glm(pa~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = "binomial")
summary(glm.tot.pa)
```

Lorsque l'on effectue le `summary` du modèle complet, on obtient un warning qui nous dit qu'une ou plusieurs observations du jeu de données sont prédites à 0 ou 1, ce qui ne veut pas forcément dire que la régression logistique est mauvaise. On rappelle que dans notre jeu de données on avait des proportions nulles. On décide donc d'ignorer le warning.
La sortie du `summary` nous permet de voir quelles variables sont significatives pour le test de Wald. Celui-ci permet de tester la nullité d'un coefficient : $H_0 : \beta_j=0$. Sous $H_0$, on a : $$\sqrt{n}(I(\hat{\beta}))_{jj}^{\frac{1}{2}}(\hat{\beta}_j)=\frac{\hat{\beta}_j}{\sqrt{\hat{Var}(\hat{\beta}_j)}}\sim N(0,1)$$ Ici, les variables significatives semblent être WatrCont, SubsDens et Topo.

On peut alors tester un modèle uniquement avec celles-ci :

```{r, echo=F}
glm.best.pa <- glm(pa~WatrCont + SubsDens + Topo, data=mites, family = "binomial")
summary(glm.best.pa)
```

Ce modèle semble mieux car les variables sont toutes vraiment significatives.

Afin de s'assurer de la significativité du retrait des autres variables, nous pouvons réaliser un test de modèles emboîtés. L'idée est de tester le modèle complet $$Y=\beta_0 + \beta_1 X_1 +...+\beta_p X_p + \epsilon$$ contre le sous-modèle $$Y=\beta_0 + \beta_1 X_1 +...+\beta_{p-q} X_{p-q} + \epsilon$$ dans lequel on n'a pas pris en compte les $q$ dernières variables. Cela revient à tester dans le modèle global $H_0 : \beta_{p-q+1}=...=\beta_p=0$ contre $H_1$ : le contraire. Si $rg(X)=p$ et $\epsilon \sim N(0,\sigma^2 I_n)$ alors sous $H_0$ :\
$$F=\frac{n-p}{q} \frac{SCR_c-SCR}{SCR} \sim F(q,n-p)$$ où $SCR$ est la somme des carrés des résidus dans le modèle complet et $SCR_c$ la somme des carrés des résidus du sous-modèle. On l'effectue avec la fonction `stats::anova`.

```{r, echo=F}
anova(glm.best.pa, glm.tot.pa, test = 'Chisq')
```

La grande p-value nous indique que les variables présentes dans le plus grand modèle mais pas dans les plus petit ne sont pas significatives. Avant de confirmer le choix de ce modèle, regardons les valeurs du critère BIC (critère explicatif) pour chacun des deux modèles.

```{r, echo=F}
kable(data.frame(BIC(glm.best.pa),BIC(glm.tot.pa)), col.names = c("Petit modèle", "Grand modèle"), caption = "BIC", position="H")
```

En accord avec les résultats précédents, le critère BIC est inférieur pour le petit modèle. Cela confirme notre choix de garder uniquement les variables `WatrCont`, `SubsDens` et `Topo.`

Nous pouvons comparer ce modèle obtenu avec celui déterminé par une sélection automatique.

-   **Méthode de sélection automatique**

Nous effectuons une sélection stepwise descendante, toujours avec le critère BIC (option `k=log(n)` dans la fonction `step`). Celle-ci part du plus gros modèle et tente d'enlever les variables les moins significtives afin de retrouver le meilleur modèle.

```{r, include=F}
n <- nrow(mites)
glm.tot.pa.step <- step(glm.tot.pa, direction = "backward", k = log(n))
```


```{r, echo=F}
cat("Modèle retenu par la fonction step : ")
print(glm.tot.pa.step)
```

La sélection automatique retient également le modèle contenant les variables `WatrCont`, `SubsDens` et `Topo` car c'est celui pour lequel le BIC est le plus faible.

Par conséquent, nous retenons le modèle suivant pour la suite :

```{=tex}
\begin{equation}
  logit(\hat{\pi})= - 0.583 - 0.022*WatrCont +0.173*SubsDens + 2.738*TopoHummock (\#eq:a)
\end{equation}
```

### Interprétation des coefficients

La sortie de notre modèle indique que le contenu en eau, la densité du substrat et la topographie sont associés significativement à l'occurrence de mites. Cependant, on peut également interpréter les coefficients de la pente grâce aux odds-ratio. L'odd-ratio est défini comme le rapport des probabilités d'apparition de l'évènement $Y=1$ contre $Y=0$. L'odd est la quantité $\frac{\pi_i}{1-\pi_i}$. Dans le modèle logistique, on définit plus précisément l'odd par : $$\frac{\pi_i}{1-\pi_i}=\exp(x_i^T\beta)=\exp(\beta_0+\beta_1x^i_1+...+\beta_px_i^p)$$ et si on considère deux individus $i_1$ et $i_2$ dont la valeur des covariables ne diffère que de 1 pour la $j$-ième covariable, soit $x_{i_1}^j-x_{i_2}^j=1$, on calcule l'odd-ratio avec : $$\frac{\pi_{i_1}}{1-\pi_{i_1}}/\frac{\pi_{i_2}}{1-\pi_{i_2}} = \exp(\beta_j)$$
On dira alors qu'une augmentation de 1 de la variable $j$ entraîne une multiplication de l'odds-ratio de $\exp(\beta_j)$.


```{r, echo=F}
kable(data.frame(exp(glm.best.pa$coefficients),row.names=c("Intercept", "WatrCont", "SubsDens", "TopoHummock")), col.names=c("Odds-ratios"), caption="Odds-ratios du modèle expliquant pa", position ="H")
```

Grâce à ces coefficients, nous pouvons déterminer le pourcentage de probabilité de la présence de mites, lorsque que l'on augmente l'une des variables. En effet, pour chaque augmentation d'une unité du contenu en eau (`WatrCont`), nous avons $2.2$% ($0.978*100 - 100$) de risque en moins d'avoir une présence de mites, tandis ce que pour une unité de densité du substrat (`SubsDens`) en plus, le risque est plus élevé de $18$%. Enfin, lorsque de la topographie est de type Hummock, il y a énormément de probabilité d'avoir des mites, pusique celle-ci augmente de plus de $1400$%.

### Validation du modèle

Afin de valider notre modèle, nous allons à présent évaluer son ajustement, ainsi que ses pouvoirs explicatifs et de classfication.

-   **Evaluation de l'ajustement du modèle**

    -   Effet levier

    Dans un premier temps, nous pouvons nous intéresser aux points leviers, c'est à dire les points qui influencent fortement sur leur propre estimation.

    ```{r, echo=F, fig.cap="Points leviers"}
    p <- length(glm.best.pa$coefficients)
    n <- nrow(mites)
    plot(influence(glm.best.pa)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    On remarque sur le graphique précédent que quatre observations peuvent être considérées comme points leviers. Une connaissance plus approfondie du jeu de données nous permettrait de comprendre plus en détails ces résultats.

    -   Points influents

    Regardons ensuite les points influents sur le modèle. Ce sont des points importants car leur présence joue beaucoup sur l'estimation des coefficients. Pour les déterminer, on peut représenter leur distance de Cook, c'est à dire la distance entre le vecteur des coefficients estimés avec toutes les observations et celui estimé avec toutes les observations sauf une.

    ```{r, echo=F, fig.cap="Points influents"}
    plot(cooks.distance(glm.best.pa), type="h", ylab="Distance de Cook")
    ```

    Sur le graphique obtenu, on remarque notamment l'observation 15 qui a une distance de Cook nettement supérieure aux autres. Cela signifie que c'est un point influent. Là encore, il serait necessaire de connaître plus en détails notre jeu de données pour pouvoir l'expliquer et notammant déterminer si c'est un point levier, un point abberrant ou les deux. Nous pouvons tout de même remarquer qu'il figurait déja dans les points à effet levier relevés précédemment.

    -   Analyse des résidus

    Afin de tester la qualité de l'ajustement de notre modèle, on peut utiliser le test de Pearson basé sur les résidus de Pearson. Ce test a pour hypothèse nulle $H_0$ : le modèle ajuste correctement les données. On définit les résidus de Pearson comme $\hat{r}_{P,i}=\frac{y_i-\hat{y_i}}{\sqrt{Var(\hat{y_i})}}$ et sous $H_0$, on a $\sum_{i=1}^{n}\hat{r}_{P,i}^2 \sim \chi^2(n-(p+1))$.

    ```{r, echo=F}
    res <- sum(residuals(glm.best.pa, type = "pearson")^2)
    ddl <- df.residual(glm.best.pa)
    pvalue <- 1 - pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value étant très grande, on admet que le modèle est très bien adapté aux données. On peut tout de même visualiser ces résidus de Pearson.
\  

    ```{r, echo=F, fig.cap="Résidus de Pearson"}
    res2 <- residuals(glm.best.pa, type="pearson")
    plot(res2, ylab="Residuals")
    abline(h=c(-2,2), col=2)
    ```

    Nous remarquons deux individus ayant des résidus élevés. Une nouvelle fois, nous retrouvons l'individu 15. Il serait peut-être judicieux de l'enlever pour la suite. Cependant, le jeu de données n'étant pas très grand, nous décidons de le conserver afin de ne pas le réduire davantage.

    -   Test d'Hosmer-Lemeshow

    Par ailleurs, on peut également effectuer un test d'Hosmer-Lemeshow permettant d'évaluer la pertinence d'un modèle de régression logistique pour lequel on a des données individuelles (pour plus d'informations voir [@Hosmer2000])

    ```{r, echo=F}
    pvalue <- hoslem.test(as.numeric(as.character(mites$pa)),fitted(glm.best.pa))$p.value
    cat("p-value pour le test de Hosmer-Lemeshow : ", pvalue)
    ```

    De même, la très grande p-value nous indique que le modèle est bien adapté aux données.

    Avant de valider définitivement le choix de ce modèle, il faut évaluer son pouvoir explicatif ainsi que son pouvoir de classification.

-   Evaluation du pouvoir explicatif du modèle

Afin d'évaluer le pouvoir explicatif de notre modèle, on peut calculer son Pseudo-$R^2$. Il en existe plusieurs. Ici, nous allons déterminer celui de McFadden (1973) correspondant au quotient entre la différence des déviances nulles et résiduelles et la déviance nulle.

```{r, echo=F}
pseudoR2 <- ((glm.best.pa$null.deviance-glm.best.pa$deviance)/glm.best.pa$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

D'après ce résultat, on en conclut que notre modèle explique plus de $62.2$% des données. Regardons ensuite s'il est capable de bien classer ces données.

-   Evaluation du pouvoir de classification du modèle

Pour évaluer le pouvoir de classification de notre modèle, nous allons juger ses prédictions grâce à une courbe ROC. Afin de ne pas prédire sur des individus ayant servis à la construction du modèle, nous effectuons une validation croisée. En effet, pour chaque individu $i$, nous estimons le modèle sans cet individu puis nous prédisons la valeur de `pa` pour celui-ci. Nous obtenons alors les prédictions de `pa` pour tous les individus et nous pouvons tracer la courbe ROC.

```{r, echo=F, fig.cap="Courbe ROC"}
pred <- 1:n
for (i in 1:n){
  # On estime le modèle sans i (on l'enlève lorsqu'on estime le modèle)
  fit <- glm(pa~WatrCont + SubsDens + Topo, data=mites, family="binomial", subset=-i)
  # On prédit la proba asscoiée à i
  pred[i] <- predict(fit,mites[i,-2], type="response")
}

pr <- prediction(pred,mites$pa)
roc <- performance(pr, measure="tpr",x.measure="fpr")
plot(roc)
```

Cette courbe résume le taux de vrais positifs en fonction du taux de faux positifs. Elle est d'autant meilleure qu'elle s'éloigne de la diagonale. Ici, nous avons une courbe largement au dessus de la diagonale. Nous pouvons d'ailleurs calculer son aire sous la courbe (AUC).

```{r, echo=F}
perf <- performance(pr, "auc")
cat("AUC : ", perf@y.values[[1]])
```

L'aire sous la courbe est proche de 1 : le pouvoir prédictif de notre modèle est alors très bon.

Enfin, nous pouvons résumer les bonnes et mauvaises prédictions de notre modèle grâce à une matrice de confusion. Pour cela, il est intéressant de déterminer en amont le seuil à partir duquel on admet la présence de mites. On trace alors la courbe du taux d'erreur en fonction des différents seuils.

```{r, echo=F, fig.cap="Taux d'erreur en fonction du seuil"}
res.err <- performance(pr,measure = "err")
plot(res.err)
```

Nous remarquons que le taux d'erreur le plus faible est pour un seuil de 0.5. C'est alors le seuil que nous allons choisir pour étblir notre matrice de confusion.

```{r, echo=F}
table(pred>0.5, mites$pa)
```

D'après cette matrice, nous avons prédit quarante fois l'abscence de mites et vingt-trois fois leur présence en ayant raison. Cependant, le modèle s'est trompé sept fois : cinq fois en ayant prédit l'abscence de mites à tort et deux fois leur présence à tort. Par conséquent, le taux d'erreur est de $\frac{7}{70}=0.1$. On peut en conclure que ce modèle a de bonnes qualités de classification.

**Conclusion :** Nous en déduisons que le modèle \@ref(eq:a) de régression logistique expliquant l'occurrence de mites grâce au contenu en eau, à la densité du substrat et à la topographie obtient de bons résultats, tant sur son ajustement que sur son pouvoir de classification.

\newpage

## GLM Régression Logistique sur les données agrégées (prop)

**Objectif** : expliquer la fréquence relative `prop`, déterminer le meilleur modèle pour cette variable réponse, interpréter les coefficients, puis évaluer l'ajustement de ce modèle.

La variable `prop` est, comme son nom l'indique, une variable en proportion. Malgré qu'il ne s'agisse pas d'une variable binaire, ce cas est proche d'une régression logistique. En effet, cette fois-ci, au lieu de prédire une valeur binaire échec ou succès (0 ou 1), on veut prédire la proportion de succès $P_i=Y_i/n_i$. Dans notre cas, la valeur $n_i$ est la variable `totalabund` qui représente le nombre total de mites, et la variable $Y_i$ est le nombre de succès, ici le nombre de mites qui sont des Galumna (représenté par la variable `Galumna` elle-même). En clair, nous avons : `prop` = `Galumna/totalabund`. On a $\mathbb{E}(Y_i)=n_i \pi_i$ et donc $\mathbb{E}(P_i)=\pi_i$. Et on modélise les probabilités $\pi_i$ par $g(\pi_i)=x_i^T \beta$ où $x_i$ est le vecteur des variables explicatives, $\beta$ est un vecteur de paramètres et $g$ est la fonction de lien. La fonction de lien pour la régression logistique est la fonction $logit$.

Sous R, on peut utiliser la fonction `glm` de manière semblable à lorsque l'on fait une régression logistique classique, mais en précisant cette fois des poids a priori.

### Recherche du meilleur modèle

On suivra la même méthode que dans la partie précédente pour chercher le meilleur modèle : une recherche à la main dans un premier temps, puis une méthode de sélection automatique.

-   **Méthode "à la main"**

```{r, echo=F}
prop.reg.tot <- glm(prop~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = "binomial", weights = totalabund)
summary(prop.reg.tot)
```

On voit qu'il n'y que très peu de variables significatives. On créé un modèle avec les deux variables les plus significatives du modèle complet : `WatrCont` et `SubsDens`.

```{r, echo=F}
prop.reg.2 <- glm(prop~WatrCont + SubsDens, data=mites, family = "binomial", weights = totalabund)
summary(prop.reg.2)
```

Cette fois-ci on voit que toutes les variables, ainsi que la constante sont significatives pour le test de Wald. Pour rappel, ce test permet de tester la nullité d'un coefficient : $H_0 : \beta_j=0$.

-   **Méthode de sélection automatique**

On effectue maintenant une recherche automatique du meilleur modèle en partant du plus gros modèle et en se basant sur le critère BIC, critère explicatif.

```{r, include=F}
prop.reg.back<-step(prop.reg.tot, direction="backward", k=log(nrow(mites)))
```

```{r, echo=F}
print("Modèle retenu par la fonction step: ")
print(prop.reg.back)
```

Avec une procédure de sélection backward, on garde 3 variables : `WatrCont`, `SubsDens` et `Topo`.

Pour cette étude, on se place dans un cadre plus explicatif que prédictif. On décide donc de comparer ces trois modèles avec le BIC :

```{r, echo=F}
kable(data.frame(BIC(prop.reg.tot),BIC(prop.reg.2),BIC(prop.reg.back)), col.names = c("Modèle complet", "Modèle 2 variables", "Modèle 3 variables"), caption = "BIC des différents modèles", position="H")
```

Le modèle à trois variables est celui qui minimise le BIC. On peut tout de même effectuer un test de modèles emboîtés afin de regarder si la troisième variable `Topo` est bien significative. On teste donc le modèle à 2 variables contre celui à 3 variables.

```{r, echo=F}
anova(prop.reg.2, prop.reg.back,test="Chisq")
```

La p-value est inférieure à 0.05 donc la variable `Topo` est significative.

On effectue un summary du modèle à 3 variables.

```{r, echo=F}
prop.reg.best <- prop.reg.back
summary(prop.reg.best)
```

On remarque que toutes les variables sont significatives d'après le test de Wald.

On retient finalement le modèle suivant :\
\begin{equation}
logit(\hat{\pi})=-4.23 -0.006*WatrCont+0.029*SubsDens+0.729*TopoHummock (\#eq:b)
\end{equation}

### Interprétation des coefficients

On peut interpréter les coefficients du modèle pour nos trois variables grâce à leurs odds-ratios. L'odd-ratio pour la variable $j$ est égal à $\exp(\beta_j)$.

```{r, echo=F}
kable(data.frame(exp(prop.reg.best$coefficients),row.names=c("Intercept", "WatrCont", "SubsDens", "TopoHummock")), col.names=c("Odds-ratios"), caption="Odds-ratios du modèle expliquant prop", position="H")
```

L'odd-ratio pour la variable `WatrCont` est inférieur à 1. De plus, sa p-value associée au test de Wald est inférieure à 5% donc on peut dire que la proportion de mites est significativement moins élevée si la quantité d'eau dans le sol augmente. L'odd-ratio de `SubsDens` est quant à lui supérieur à 1, et sa p-value pour le test de Wald inférieure à 5%. Cela signifie que la proportion de mites sera significativement plus importante si la densité du substrat augmente.\
Enfin, l'odd-ratio pour `Topo` vaut 2.07 et sa p-value est inférieure à 5%. Donc on peut conclure que si la topographie est de type Hummock, la proportion de mites sera significativement plus élevée.

### Validation du modèle

Enfin, nous devons maintenant regarder si notre modèle est "bon". Nous allons, pour cela, évaluer sa qualité d'ajustement et son pouvoir explicatif.

-   **Evaluation de l'ajustement du modèle**

    -   Effet levier

    On regarde dans un premier temps les points leviers. Ce sont les points qui influencent fortement leur estimation.

    ```{r, echo=F, fig.cap="Points leviers"}
    p<-length(prop.reg.back$coefficients)
    plot(influence(prop.reg.back)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    On voit que 5 observations peuvent être déclarées comme "points leviers".

    -   Points influents

    Les points influents sont des points qui influent sur le modèle de telle sorte que si on les enlève, l'estimation des coefficients sera fortement changée. Pour les observer, on représente leur distance de Cook.

    ```{r, echo=F, fig.cap="Points influents"}
    plot(cooks.distance(prop.reg.back), type="h", ylab="Distance de Cook", main="Points influents")
    ```

    On relève plusieurs points influents. Notamment la 8ème observation, qui était déjà un point levier. Généralement, si un point est influent, il est soit levier, soit aberrant, soit les deux.

    -   Analyse des résidus

    Finalement, nous allons analyser les résidus. On effectue pour cela un test de Pearson ou test global de la qualité de l'ajustement basé sur les résidus de Pearson.

    ```{r, echo=F}
    res <- sum(residuals(prop.reg.back, type = "pearson")^2)
    ddl <- df.residual(prop.reg.back)
    pvalue <- 1-pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value est très petite. Cela signifie que d'après le test de Pearson, notre modèle n'ajuste pas très bien les données. Cependant, nous ne voyons pas comment le changer afin qu'il soit meilleur à ce niveau là. Nous continuons avec ce modèle, tout en gardant à l'esprit qu'il présente des défauts d'ajustement.

    On finit par représenter les résidus.

    ```{r, echo=F, fig.cap="Résidus de Pearson"}
    res<-residuals(prop.reg.back, type="pearson")
    plot(res, ylim=c(-3,5), ylab="Residuals")
    abline(h=c(-2,2), col=2)
    ```

    On voit que les résidus sont répartis de façon homogène autour de l'axe des abscisses. Ils sont pour la plupart proches de 0. Cependant, trois points semblent avoir une valeur de résidu élevée. Peut-être correspondent-ils à des points leviers ou aberrants observés précédemment. Nous pourrions les enlver du jeu de données afin de construire un modèle qui ajusterait encore mieux les données mais nous décidons de les garder car le jeu de données ne contient pas beaucoup d'individus (70) et nous ne voulons pas le réduire davantage. De plus, nous n'avons pas une connaissance très poussée de nos données donc nous ne pourrions pas justifier le retrait de ces individus pour notre étude.

-   **Evaluation du pouvoir explicatif du modèle**

Pour finir la validation du modèle, on calcule le pseudo-$R^2$, qui nous donne la variance expliquée par notre modèle :

```{r, echo=F}
pseudoR2 <- ((prop.reg.best$null.deviance-prop.reg.best$deviance)/prop.reg.best$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

Notre modèle explique $44$% de la variance. Ce score n'est pas très élevé. Cependant, en régression logistique, les valeurs faibles de pseudo-$R^2$ sont souvent courantes.

**Conclusion : ** Nous en déduisons que le modèle \@ref(eq:b) de régression logistique expliquant la proportion de mites Galumna grâce au contenu en eau, à la densité du substrat et à la topographie obtient de bons résultats.

\newpage

## GLM Régression Poisson sur la variable d'abondance (Galumna)

**Objectif** : modéliser l'abondance de l'espèce Galumna en fonction des caractéristiques du substrat (son contenu en eau `WatrCont` et sa densité `SubsDens`) et, si nécessaire, des autres variables environnementales.

Pour ce faire, nous allons utiliser un modèle linéaire généralisé avec une distribution adaptée à notre problème de comptage : la distribution de Poisson.

Pour une distribution Poisson $Y \sim Pois(\lambda)$ avec le lien $log$ par défaut nous avons $log(Y)=\eta(.)$ avec l'inverse de ce lien $\lambda=e^{\eta(.)}$, où $\eta(.)$ représente le prédicteur linéaire.

Comme pour la régression logistique, la régression de Poisson utilise la fonction `glm`. Il faut spécifier la famille "poisson" et (optionnellement) le lien $log$. En effet, le logarithme est la foncton de lien par défaut pour la régression de Poisson sur R.

### Recherche du meilleur modèle

-   **Méthode "à la main"**

Nous allons commencer par ajuster un modèle complet et ensuite effectuer une sélection de variables.

```{r, echo=F}
glm.tot.poi <- glm(Galumna~WatrCont + SubsDens + Topo + Shrub + Substrate, data=mites, family = poisson(link='log'))
summary(glm.tot.poi)
```

Nous observons dans le `summary` de notre modèle complet qu'il n'y a que très peu de variables significatives. On créé un modèle avec les deux variables significatives du modèle complet : `WatrCont` et `SubsDens`.

```{r, echo=F}
glm.poi <- glm(Galumna~WatrCont + SubsDens, family = poisson(link = "log"), data = mites )
summary(glm.poi)
```

On observe que toutes les variables, ainsi que la constante, sont significatives pour le test de Wald.

-   **Recherche automatique**

Avant de confirmer ce modèle, nous allons effectuer une recherche automatique du meilleur modèle en partant du plus gros modèle et en se basant sur le critère BIC, critère explicatif.

```{r, include=F}
glm.reg.back<-step(glm.tot.poi, direction="backward", k=log(nrow(mites)))
```

```{r, echo=F}
print("Modèle retenu par la fonction step : ")
print(glm.reg.back)
```

Avec une procédure de sélection backward, on garde 3 variables : `WatrCont`, `SubsDens` et `Topo`.

Nous avons donc un nouveau modèle :

```{r, echo=F}
glm.poi3 <- glm(Galumna~WatrCont + SubsDens + Topo, family = poisson(link = "log"), data = mites )
summary(glm.poi3)
```
On voit que toutes les variables sont significatives pour le test de Wald.

Pour cette étude, on se place dans un cadre plus explicatif que prédictif. On décide donc de comparer ces trois modèles avec le BIC :

```{r, echo=F}
kable(data.frame(BIC(glm.tot.poi),BIC(glm.poi),BIC(glm.poi3)), col.names = c("Modèle complet", "Modèle à 2 variables", "Modèle à 3 variables"), caption = "BIC des différents modèles", position="H")
```

Le modèle à trois variables est celui qui minimise le BIC. On peut tout de même effectuer un test de modèles emboîtés afin de regarder si la troisième variable `Topo` est bien significative. On teste donc le modèle à 2 variables contre celui à 3 variables.

```{r, echo=F}
anova(glm.poi3, glm.poi,test="Chisq")
```

La p-value est inférieure à 0.05 donc la variable `Topo` est significative.

Le modele selectionné s'écrit donc : $$log(Y)=\beta_0+\beta_1*WatrCont+\beta_2*SubsDens+\beta_3*TopoHummock$$ ce qui nous donne à l'aide du `summary` de notre modèle : \begin{equation}
log(Y)=0.710-0.008*WatrCont+0.034*SubsDens + 0.892*TopoHummock (\#eq:c)
\end{equation}

### Interprétation des coefficients

Nous allons maintenant interpréter les coefficients de notre modèle. Nous allons determiner l'effet de chaque coefficient du modèle sur le prédicteur linéaire $\eta=\beta_0+\beta_1*WatrCont+\beta_2*SubsDens+\beta_3*TopoHummock$, puis déduire l'effet sur la moyenne de réponse à partir de la fonction $\lambda=e^\eta$.

-   Le coefficient `WatrCont` qui indique le contenu en eau, indique que $\eta$ diminue de 0.008 pour chaque augmentation d'une unité de `WatrCont` si la variable SubsDens reste constante. Nous avons donc le facteur $e^{-0.008}=0.99$ qui correspond à une perte de 1% de l'abondance de Galumna par unité de contenu en eau (`WatrCont`) supplémentaire.
-   Le coefficient `SubsDens` qui indique la densité, a un changement additif de $0.023$ qui correspond au facteur $e^{0.034}=1.034585$. On en conclut que la densité n'influe pas sur l'abondance de Galumna.
-   Le coefficient `TopoHummock` qui représente la topographie de type Hummock, a un changement additif de $0.892$ qui correspond au facteur $e^0.892=2.440005$. Cela correspond a une augmentation de $250$% de l'abondance de Galumna pour une topographie de type Hummock.

### Validation du modèle

Enfin, nous passons à la phase de validation du modèle. Nous allons, pour cela, regarder s'il y a de la surdispersion dans ce modèle, puis évaluer sa qualité d'ajustement et son pouvoir explicatif.

-   **Analyse de la surdispersion**

Pour une distribution de Poisson, $Var(Y)=\mu=E(Y)$. En pratique, on constate que la variance des données dépasse souvent $\mu$, indiquant une surdispersion dans les paramètres du modèle.\
Par définition, lorsque la déviance résiduelle est supérieure au nombre de degrés résiduels, le modèle est surdispersé. On peut estimer un paramètre de surdispersion $$\phi = \frac{\text{Déviance résiduelle}}{\text{Degré de liberté résiduels}}$$. Nous avons ici $\phi = \frac{91.699}{66}=1.389379$. Sous l'hypothèse nulle de non surdispersion ($\phi=1$), la deviance $D$ suit une loi du $\chi^2$ à $(n-p)=70-4=66$ degrès de liberté. Rappelons que nous rejetons $H_0$ si $D>\chi^2_{66,0.95}$ avec, d'après la table de la loi du khi-deux, $\chi^2_{67,0.95}=85,97$. Dans le `summary` de notre modèle nous avons que $D=91.699$ pour $66$ degrès de liberté. Nous rejetons donc $H_0$ et nous concluons que nous avons de la surdispersion dans nos données. Nous savons que la non-indépendance des observations individuelles peut causer une surdispersion des données par rapport aux suppositions de la distribution de Poisson.

Pour déterminer si le $\chi^2$ diffère significativement de la valeur attendue selon la distribution de Poisson, nous pouvons calculer la probabilité d'avoir obtenu un $\chi^2$ plus élevé si le modèle est correct.

```{r, echo=F}
chisq <- sum((mites$Galumna-fitted(glm.poi3))^2/fitted(glm.poi3))
1-pchisq(chisq, df=glm.poi3$df.residual)
```

```{r,echo=F}
disp <- chisq/glm.poi3$df.residual
cat("Estimation du paramètre de dispersion : ",disp)
```

L'estimation du paramètre de dispersion est de $\hat{\phi}=1.9$, cette valeur n'est pas trop élevée. Il faut faire attention quand elle dépasse $5$. Les estimations des coefficients de la régression de Poisson demeurent valides, mais il faut multiplier leurs erreurs-types par $\sqrt{\hat{\phi}}$. Autrement dit, la surdispersion n'introduit pas de biais, mais augmente l'incertitude sur les valeurs des coefficients.

Pour corriger les erreurs-types, nous pouvons utiliser la famille `quasipoisson` dans notre modèle.

```{r, echo=F}
glm.poi4 <- glm(Galumna ~ WatrCont + SubsDens + Topo , family = quasipoisson, data = mites )
summary(glm.poi4)
```

On voit que les coefficients ne changent pas, contrairement aux erreurs-types.  

-   **Évaluation de l'ajustement du modèle**

    -   Effet levier

    On regarde une nouvelle fois les points leviers.

    ```{r, echo=F, fig.cap="Points leviers"}
    p <- length(glm.poi4$coefficients)
    n <- nrow(mites)
    plot(influence(glm.poi4)$hat, type="h", ylab="h_ii")
    abline(h=c(2*p/n, 3*p/n), col=2)
    ```

    Cinq observations peuvent être considérées comme points leviers. Une connaissance plus approfondie du jeu de données nous permettrait de comprendre plus en détail ces résultats.

    -   Points influents

    Regardons ensuite les points influents sur le modèle.

    ```{r, echo=F, fig.cap="Points influents"}
    plot(cooks.distance(glm.poi4), type="h", ylab="Distance de Cook")
    ```

    Sur le graphique obtenu, on remarque notamment l'observation 8 qui a une distance de Cook nettement supérieure aux autres. Cela signifie que c'est un point influent. Là encore, il serait necessaire de connaître plus en détails notre jeu de données pour pouvoir l'expliquer et notammant déterminer si c'est un point levier, un point abberrant ou les deux.

    -   Analyse des résidus

    Pour tester la qualité de l'ajustement du modèle, on effectue un test de Pearson.

    ```{r, echo=F}
    res <- sum(residuals(glm.poi4, type = "pearson")^2)
    ddl <- df.residual(glm.poi4)
    pvalue <- 1 - pchisq(res,ddl)
    cat("p-value pour le test de Pearson : ", pvalue)
    ```

    La p-value est très petite. Cela signifie que d'après le test de Pearson, notre modèle n'ajuste pas très bien les données. Cependant, nous ne voyons pas comment le changer afin qu'il soit meilleur à ce niveau là. Nous continuons avec ce modèle, tout en gardant à l'esprit qu'il présente des défauts d'ajustement.

-   **Evaluation du pouvoir explicatif du modèle**

Calculons maintenant le Pseudo-$R^2$ du modèle.

```{r, echo=F}
pseudoR2 <- ((glm.poi4$null.deviance-glm.poi4$deviance)/glm.poi4$null.deviance)
cat("pseudo-R2 : ", pseudoR2)
```

D'après ce résultat, on en conclut que notre modèle explique plus de $45$% des données.


Ainsi, nous en déduisons que le modèle \@ref(eq:c) de régression quasi-Poisson expliquant l'abondance de Galumna grâce au contenu en eau, à la densité du substrat et à la topographie est un bon modèle explicatif pour nos données.


**Conclusion :**\
Le but initial de notre étude était de décrire le jeu de données sur les mites Oribatid. Pour cela, nous avons commencé par faire de l'analyse descriptive des variables, individuellement, puis colectivement. Ensuite, nous avons tenté d'ajuster un modèle linéaire pour expliquer trois variables du jeu de données. Nous avons été confrontées à un problème : aucun modèle, même aorès transformation des données, ne satisfaisait toutes les hypothèses du modèle linéaire gaussien. Nous avons donc choisi d'autres distributions (non gaussiennes), mieux adaptées à nos données, et ajusté un modèle linéaire généralisé pour chacune d'entre elles. Finalement, nous obtenons trois modèles qui nous permettent d'expliquer nos trois variables. Nous jugeons nos modèles plutôt bons, bien que leur qualité globale puisse être discutée.

\newpage

# Données manquantes

## 1. Scénarios NA sur un jeu de données simulées

#### Simulations

Dans le but d'étudier différents scénarios d'imputation de données manquantes, nous commençons par simuler $n=100$ réalisations d'un vecteur gaussien $(X,Y)$ de moyenne $\mu=\begin{pmatrix} 0 \\ 0 \end{pmatrix}$ et de variance $\begin{pmatrix} 1 & 0.5 \\ 0.5 & 1 \end{pmatrix}$. Pour cela, on utilise la fonction `rmvnorm` du package `mvtnorm`.

```{r, echo=F}
n <- 100
mu <- c(0,0)
matcov <- matrix(c(1, 0.5, 0.5, 1), nrow=2, ncol = 2)
don <- rmvnorm(n=n, mean = mu, sigma = matcov)
colnames(don) <- c("X", "Y") 
```

Ensuite, nous ajoutons des données manquantes sur $Y$ selon plusieurs mécanismes, afin de les comparer par la suite.

-   Mécanisme **MCAR** (**M**issing **C**ompletely **A**t **R**andom)

Le premier mécanisme que nous utilisons est appelé MCAR. Pour celui-ci, les données sont manquantes de façon totalement aléatoire, c'est-à-dire que la probabilité d'abscence est la même pour toutes les observations. Dans notre cas, on choisit $P(M=0)=0.35$.

Voici les informations concernant du jeu de données obtenu :

```{r, echo=F}
set.seed(123)
don.ismcar = don

ismcar <- sample(c(T,F), size = n, prob = c(0.35, 0.65), replace = TRUE)
#Bernouilli trial avec TRUE

don.ismcar[ismcar, "Y"] <- NA
don.ismcar <- as.data.frame(don.ismcar) # création du jeu avec 0.35 de NA pour Y
summary(don.ismcar)
```

-   Mécanisme **MAR** (**M**issing **A**t **R**andom)

Ensuite, nous utilisons le mécanisme MAR. Contrairement au précédent, la probabilité d'abscence est liée à une ou plusieurs autres variables observées. Elle ne dépend cependant pas des valeurs manquantes. Ici, on choisit $P(M=0\vert{X})=\mathcal{B}(\phi(1.2X-0.5))$ où $\phi(.)$ désigne la fonction de répartition d'une loi normale $\mathcal{N}(0,1)$.

On obtient un jeu de données ayant les caractéristiques suivantes pour chacune de ses variables :

```{r, echo=F}
set.seed(123)

don.ismar <- don
ismar <- sapply(don.ismar[,"X"], FUN=function(xx){
  prob <- pnorm(1.2*xx-.5) #Fonction de rep
  res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
  return (res)
})

don.ismar[ismar,"Y"] <- NA #Les valeurs de Y tq ismar = T sont mises à NA
don.ismar <- as.data.frame(don.ismar)
summary(don.ismar)
```

-   Mécanisme **MNAR** (**M**issing **N**ot **A**t **R**andom)

Enfin, nous utilisons le mécanisme MNAR. Cette fois-ci, les données sont manquantes de façon non aléatoire, c'est-à-dire qu'elles dépendent des variables en question. En effet, la probabilité qu'une valeur soit manquante dépend d'une ou plusieurs données non observées pour les autres variables, ici $X$. Dans notre cas, on a $P(M=0\vert{Y})=\mathcal{B}(\phi(1.2X-0.5))$, où $\phi(.)$ désigne toujours la fonction de répartition d'une loi normale $\mathcal{N}(0,1)$.

Le jeu de données obtenu contient toujours deux variables vérifiant les informations suivantes :

```{r, echo = F}
set.seed(123)

don.ismnar <- don
ismnar <- sapply(don.ismnar[,"Y"], FUN=function(xx){
  prob <- pnorm(1.2*xx-.5) #Fonction de rep
  res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
  return (res)
})

don.ismnar[ismnar,"Y"] <- NA #Les valeurs de Y tq ismar = T sont mises à NA
don.ismnar <- as.data.frame(don.ismnar)
summary(don.ismnar)
```

Notons que ce dernier type de données manquantes est plus complexe à traiter par la suite.

#### Analyse

Maintenant que nous avons ajouté de trois manières différentes des valeurs manquantes sur la variable $Y$, nous allons analyser les jeux de données obtenus. Pour cela, nous commençons par les visualiser en entier. Nous utilisons la fonction `vis_miss` du package `visdat`.

```{r, echo = F, warning = F}
print("MCAR")
vis_miss(don.ismcar)
print("MAR")
vis_miss(don.ismar)
print("MNAR")
vis_miss(don.ismnar)
```

Nous remarquons que les pourcentages de données manquantes sont équivalents. Cependant, elles ne sont pas réparties de la même façon dans les différents cas.

Nous pouvons alors regarder plus en détails comment sont reparties les données manquantes de $Y$ selon les valeurs de $X$ pour chacun des jeux de données. Pour cela, nous utilisons la fonction `geom_miss_point` disponible dans `ggplot`.

```{r, echo = F}
ggplot(data = don.ismcar) +
aes(x = X, y = Y) + geom_miss_point() + labs(title="MCAR") + theme(plot.title = element_text(size = 15, color = "black", face = "bold"))

ggplot(data = don.ismar) +
aes(x = X, y = Y) + geom_miss_point() + labs(title="MAR") + theme(plot.title = element_text(size = 15, color = "black", face = "bold"))

ggplot(data = don.ismnar) +
aes(x = X, y = Y) + geom_miss_point() + labs(title="MNAR") + theme(plot.title = element_text(size = 15, color = "black", face = "bold"))
```

Nous remarquons que pour la deuxième configuration, les valeurs manquantes sont plutôt associées à un $X$ positif, tandis que dans les deux autres cas, elles sont reparties dans les négatifs également.

***NOTE à enlever** : dans les "outils" dont elle parle dans son diapo, la moitié on ne peut pas les utiliser car on a que 2 var et qu'une qui a des valeurs manquantes : par ex gg_miss_var sert à rien avec 2 var car compare juste les pourcentages de Na et gg_miss_upset nécessite au moins deux var avec des Na.* -\> elle est relou wesh

#### Estimation de la moyenne de $Y$ et de l'intervalle de confiance associé

Nous voulons à présent étudier nos jeux de données et déterminer la moyenne de la variable $Y$, ainsi que l'intervalle de confiance associé pour chacun d'entre eux. Nous pourrons ensuite comparer les résultats obtenus.

Pour ce faire, plusieurs méthodes seront utilisées. En effet, dans un premier temps, nous allons utiliser uniquement les données non manquantes de la variable. Puis, nous testerons différentes méthodes d'imputation pour les valeurs non renseignées.

-   Cas complets

Comme évoqué, nous débutons par une analyse des cas complets. Cela consiste à supprimer les observations pour lesquelles la valeur de $Y$ est manquante. C'est la méthode la plus simple. Cependant, elle est déconseillée lorsque le nombre de données manquantes est trop élevé car nous perdrions trop d'informations. De plus, pour les jeux de données où les valeurs manquantes ne sont pas type MCAR, retirer des observations va induire un biais dans l'analyse puisque l'ensemble des observations pour lesquelles des données sont manquantes n'est pas forcément représentatif de l'échantillon initial.

Dans notre cas, nous avons environ $30$% de Na dans nos différents jeux de données. De plus, deux d'entre eux comportent des valeurs manquantes qui ne sont pas de type MCAR. Il ne serait donc pas judicieux de procéder ainsi. Cependant, l'objectif ici étant de comparer plusieurs méthodes, nous allons tout de même le réaliser. Pour cela, nous utilisons la fonction `na.omit`.

```{r, echo = F}
don.ismcar.cc <- na.omit(don.ismcar)
don.ismar.cc <- na.omit(don.ismar)
don.ismnar.cc <- na.omit(don.ismnar)
```

Nous pouvons regarder la dimension de nos nouveaux jeux de données "complets".

```{r, echo = F}
dfMCAR <- data.frame(dim(don.ismcar.cc), row.names = c('Observations', "Variables"))
colnames(dfMCAR) = "MCAR"

dfMAR <- data.frame(dim(don.ismar.cc), row.names = c('Observations', "Variables"))
colnames(dfMAR) = "MAR"

dfMNAR <- data.frame(dim(don.ismnar.cc), row.names = c('Observations', "Variables"))
colnames(dfMNAR) = "MNAR"

kable(data.frame(cbind(dfMCAR,dfMAR,dfMNAR)),position="H", caption = "Dimension du jeu de données complet")
```

La dimension initiale était de $100$ observations. Après suppression, nous en gardons nettement moins. Rappelons que les lignes retirées ne sont pas les mêmes pour chacun des jeux de données car les valeurs manquantes ne se situaient pas au même endroit.

Nous pouvons à présent calculer la moyenne de $Y$. Pour cela, nous réalisons une régression simple sur la variable, grâce à la fonction `lm` de la librairie `stats`, et nous récupérons l'intercept. Les moyennes obtenues sont présentes dans le tableau suivant :

```{r, echo = F}
lm.mcar.cc <- lm(don.ismcar.cc$Y~1)
lm.mar.cc <- lm(don.ismar.cc$Y~1)
lm.mnar.cc <- lm(don.ismnar.cc$Y~1)

mean.mcar.cc <- lm.mcar.cc$coefficients
mean.mar.cc <- lm.mar.cc$coefficients
mean.mnar.cc <- lm.mnar.cc$coefficients

kable(data.frame(mean.mcar.cc,mean.mar.cc,mean.mnar.cc), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y des cas complets")
```

On remarque que les moyennes sont très différentes selon les jeux de données. Seule la première se rapproche de $0$, la vraie valeur.

Regardons maintenant les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.cc)), data.frame(confint(lm.mar.cc)), data.frame(confint(lm.mnar.cc)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y des cas complets")
```

Nous remarquons ici aussi que le premier jeu de données, correspondant au type MCAR, est le seul a avoir de "bons" résultats. En effet, contrairement aux autres, le $0$ est présent dans l'intervalle. Cela confirme ce que nous avions évoqué précédemment sur le biais que provoque cette méthode lorsque les données manquantes ne sont pas de type MCAR.

-   Imputation simple par la moyenne

Pour éviter de supprimer les observations où se trouvent des données manquantes, on peut décider d'imputer des valeurs à ces endroits. Plusieurs méthodes existent pour déterminer la valeur à indiquer. La première que nous testons consiste à imputer le Na par la moyenne de la variable. Toutes les données manquantes seront donc "remplacées" par la même valeur.

Pour le calcul de la moyenne de la variable ensuite, nous obtenons forcément les mêmes résultats qu'auparavant :

```{r, echo = F}
don.ismcar.im <- na.aggregate(don.ismcar, FUN = mean)
don.ismar.im <- na.aggregate(don.ismar, FUN = mean)
don.ismnar.im <- na.aggregate(don.ismnar, FUN = mean)

lm.mcar.im <- lm(don.ismcar.im$Y~1)
lm.mar.im <- lm(don.ismar.im$Y~1)
lm.mnar.im <- lm(don.ismnar.im$Y~1)

mean.mcar.im <- lm.mcar.im$coefficients
mean.mar.im <- lm.mar.im$coefficients
mean.mnar.im <- lm.mnar.im$coefficients

kable(data.frame(mean.mcar.im,mean.mar.im,mean.mnar.im), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après imputation simple par la moyenne")
```

Regardons ce que donnent les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.im)), data.frame(confint(lm.mar.im)), data.frame(confint(lm.mnar.im)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après imputation simple par la moyenne")
```

Les intervalles de confiance ont changés. Il y a toujours uniquement dans le premier cas (MCAR) que $0$ se situe dans l'intervalle. De plus, les autres s'éloignent de cette valeur. Cependant, leur longueur a diminué. Ainsi, ils offrent plus de précision dans leur estimation.

-   Imputation simple par régression stochastique

La deuxième méthode d'imputation que nous testons est l'imputation par régression stochastique. Celle-ci consiste à "remplacer" une donnée non observée par une valeur prédite obtenue en régressant la variable manquante sur d'autres variables, à laquelle on ajoute une valeur résiduelle aléatoire. Cela permet de préserver les relations entre les variables, mais aussi d'avoir l'avantage d'une composante aléatoire.

Concrètement, pour réaliser cette méthode, il nous faut tirer aléatoirement une valeur autour de celle prédite par le modèle de régression. Pour cela, nous utilisons la fonction `mice` avec la méthode `norm.nob`.

Nous visualisons ensuite les données pour s'assurer que la distribution des données imputées est globalment la même que celle des données initiales.

```{r, echo = F}
don.ismcar.irs <- complete(mice(don.ismcar, method = "norm.nob", m = 1, print = FALSE))
don.ismar.irs <- complete(mice(don.ismar, method = "norm.nob", m = 1, print = FALSE))
don.ismnar.irs <- complete(mice(don.ismnar, method = "norm.nob", m = 1, print = FALSE))

plot(don.ismcar.irs$X[!is.na(don.ismcar$Y)], don.ismcar.irs$Y[!is.na(don.ismcar$Y)],  
     xlim = c(0, 3), ylim = c(- 5, 10),
     main = "Imputation simple par régression stochastique - MCAR",
     xlab = "X", ylab = "Y")
points(don.ismcar.irs$X[is.na(don.ismcar$Y)], don.ismcar.irs$Y[is.na(don.ismcar$Y)],   
       col = "red")
abline(lm(Y ~ X, don.ismcar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))

plot(don.ismar.irs$X[!is.na(don.ismar$Y)], don.ismar.irs$Y[!is.na(don.ismar$Y)],  
     xlim = c(0, 3), ylim = c(- 5, 10),
     main = "Imputation simple par régression stochastique - MAR",
     xlab = "X", ylab = "Y")
points(don.ismar.irs$X[is.na(don.ismar$Y)], don.ismar.irs$Y[is.na(don.ismar$Y)],   
       col = "red")
abline(lm(Y ~ X, don.ismar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))

plot(don.ismnar.irs$X[!is.na(don.ismnar$Y)], don.ismnar.irs$Y[!is.na(don.ismnar$Y)],  
     xlim = c(0, 3), ylim = c(- 5, 10),
     main = "Imputation simple par régression stochastique - MNAR",
     xlab = "X", ylab = "Y")
points(don.ismnar.irs$X[is.na(don.ismnar$Y)], don.ismnar.irs$Y[is.na(don.ismnar$Y)],   
       col = "red")
abline(lm(Y ~ X, don.ismnar), col = "blue", lwd = 1.5)
legend("topleft",                                        
       c("Valeurs observées", "Valeurs imputées", "Régression Y ~ X"),
       pch = c(1, 1, NA),
       lty = c(NA, NA, 1),
       col = c("black", "red", "blue"))
```

Les différentes imputations semblent réalistes car elles suivent une distribution semblable à celles des données déjà existantes.

Nous calculons ensuite les moyennes et obtenons les résultats suivants :

```{r, echo = F}
lm.mcar.irs <- lm(don.ismcar.irs$Y~1)
lm.mar.irs <- lm(don.ismar.irs$Y~1)
lm.mnar.irs <- lm(don.ismnar.irs$Y~1)

mean.mcar.irs <- lm.mcar.irs$coefficients
mean.mar.irs <- lm.mar.irs$coefficients
mean.mnar.irs <- lm.mnar.irs$coefficients

kable(data.frame(mean.mcar.irs,mean.mar.irs,mean.mnar.irs), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après imputation simple par régression stochastique")
```

Cette fois-ci, c'est le deuxième cas (MAR) qui obtient le meilleur résultat car la moyenne obetnue est très proche de celle attendue.

Nous pouvons regarder ce que donnent les intervalles de confiance associés :

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.irs)), data.frame(confint(lm.mar.irs)), data.frame(confint(lm.mnar.irs)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"), position="H", caption = "Intervalle de confiance associé à la moyenne de Y après imputation simple par régression stochastique")
```

Les intervalles de confiance obtenues avec cette méthode d'imputation semblent meilleurs que ceux des méthodes précédemment testées. En effet, les deux premiers contiennent $0$ et le troisième s'en rapproche. Cependant, leur longueur est plus grande que pour l'imputation avec la moyenne. Ils sont donc moins précis.

-   20 imputations simples par régression stochastique

La méthode précédente ayant des résultats encourageants, nous souhaitons poursuivre son étude. En effet, une valeur unique ne pouvant pas refléter l'incertitude sur la prédiction, nous allons tester de renouveller vingt fois l'imputation. Pour cela, nous utilisons toujours la fonction `mice` mais en précisant l'argument `m=20`.

Nous obtenons alors $20$ jeu de données imputés. Regardons les valeurs pour chacun d'eux, dans les trois cas que nous étudions :

```{r, echo = F}
don.ismcar.irs20 <- mice(don.ismcar, method = "norm.nob", m = 20, print = FALSE)
don.ismar.irs20 <- mice(don.ismar, method = "norm.nob", m = 20, print = FALSE)
don.ismnar.irs20 <- mice(don.ismnar, method = "norm.nob", m = 20, print = FALSE)

stripplot(don.ismcar.irs20, pch = 20, cex = 1.2, main = "MCAR")
stripplot(don.ismar.irs20, pch = 20, cex = 1.2, main = "MAR")
stripplot(don.ismnar.irs20, pch = 20, cex = 1.2, main = "MNAR")
```

Les données imputées se trouvant en rouge, nous remarquons que chaque imputation est différente, mais respecte la distribution des données initiales.

Il faut maintenant combiner les données en utilisant la fonction `pool` et on récupère la moyenne obtenue.

```{r, echo  = F}
fit_mcar <- with(don.ismcar.irs20, lm(Y~1))
fitpool_mcar <- pool(fit_mcar)
mean.mcar.irs20 <- fitpool_mcar$pooled[3]

fit_mar <- with(don.ismar.irs20, lm(Y~1))
fitpool_mar <- pool(fit_mar)
mean.mar.irs20 <- fitpool_mar$pooled[3]

fit_mnar <- with(don.ismnar.irs20, lm(Y~1))
fitpool_mnar <- pool(fit_mnar)
mean.mnar.irs20 <- fitpool_mnar$pooled[3]

kable(data.frame(mean.mcar.irs20,mean.mar.irs20,mean.mnar.irs20), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après combinaison des 20 imputations par régression stochastique")
```

Une nouvelle fois, le mécanisme MAR obtient le meilleur résultat. Il est même plus proche de $0$ que lorsque nous avions réalisé qu'une seule imputation. C'est le cas aussi pour le mécanisme MCAR. En revanche, pour MNAR, on s'en est bien éloigné. Cependant, l'incertitude de l'imputation des données manquantes étant prise en compte, on peut considérer que ce sont des résultats plus "justes".

Regardons les intervalles de confiance associés :

PAS REUSSI avce pool

```{r, echo = F}
IC <- rbind(data.frame(confint(lm.mcar.irs20)), data.frame(confint(lm.mar.irs20)), data.frame(confint(lm.mnar.irs20)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"))
```

Les conclusions sont les mêmes que pour les moyennes. En effet, les intervalles sont légèrement plus grands mais sont sensiblement répartis de la même façon par rapport à $0$.

***NOTE** : pas sûre de moi pour la régression multiple, je sais pas si mice gère directement les 20 imputations ou si on doit faire qqch après.*

-   Imputation multiple par régression

Enfin, nous allons tester de réaliser une nouvelle fois une imputation multiple. En effet, nous réalisons vingt imputations, mais cette fois-ci par régression linéaire. Nous le précisons dans le code en indiquant `method=norm` dans la fonction `mice`.

Nous obtenons les distributions suivantes :

```{r, echo = F}
don.ismcar.ir20 <- mice(don.ismcar, method = "norm", m = 20, print = FALSE)
don.ismar.ir20 <- mice(don.ismar, method = "norm", m = 20, print = FALSE)
don.ismnar.ir20 <- mice(don.ismnar, method = "norm", m = 20, print = FALSE)

stripplot(don.ismcar.ir20, pch = 20, cex = 1.2, main = "MCAR")
stripplot(don.ismar.ir20, pch = 20, cex = 1.2, main = "MAR")
stripplot(don.ismnar.ir20, pch = 20, cex = 1.2, main = "MNAR")
```

Nous calculons enuit les moyennes et nous obtenons les résulatst suivants :

```{r, echo = F}
fit_mcar <- with(don.ismcar.ir20, lm(Y~1))
fitpool_mcar <- pool(fit_mcar)
mean.mcar.ir20 <- fitpool_mcar$pooled[3]

fit_mar <- with(don.ismar.ir20, lm(Y~1))
fitpool_mar <- pool(fit_mar)
mean.mar.ir20 <- fitpool_mar$pooled[3]

fit_mnar <- with(don.ismnar.ir20, lm(Y~1))
fitpool_mnar <- pool(fit_mnar)
mean.mnar.ir20 <- fitpool_mnar$pooled[3]

kable(data.frame(mean.mcar.ir20,mean.mar.ir20,mean.mnar.ir20), col.names = c("MCAR", "MAR", "MNAR"), row.names = FALSE, position="H", caption = "Moyenne de Y après combinaison des 20 imputations par régression")
```

C'est encore pour le deuxième jeu de données que l'on obtient le meilleur résultat. L'imputation par régression semble donc mieux adaptée pour le mécanisme MAR.

Regardons si cela se confirme pour les intervalles de confiance associés aux moyennes :

PAS REUSSI non plus du coup (idem que stochastique)

```{r}
IC <- rbind(data.frame(confint(lm.mcar.ir20)), data.frame(confint(lm.mar.ir20)), data.frame(confint(lm.mnar.ir20)))

rownames(IC) <- c("MCAR", "MAR", "MNAR")
colnames(IC) <- c("2.5%", "97.5%")

kable(data.frame(IC), col.names =c("2.5%", "97.5%"))
```

Nous observons que la vraie valeur est située dans les deux premiers intervalles uniquement. De plus, le deuxième semble légèrement plus centré autour de cette valeur.

#### Comparaison des stratégies

Afin de comparer chacune des méthodes précédentes, nous allons recommencer toutes les procédures $200$ fois. Cela nous permettra d'estimer le biais commis sur l'estimation de la moyenne ainsi que la longueur moyenne et le taux de couverture de l'intervalle de confiance.

```{r, echo = F}
mean.mcar.cc <- c(1:200)
mean.mar.cc <- c(1:200)
mean.mnar.cc <- c(1:200)

mean.mcar.im <- c(1:200)
mean.mar.im <- c(1:200)
mean.mnar.im <- c(1:200)

mean.mcar.irs <- c(1:200)
mean.mar.irs <- c(1:200)
mean.mnar.irs <- c(1:200)

IC.mcar.cc <- matrix(ncol = 2, nrow = 200)
IC.mar.cc <- matrix(ncol = 2, nrow = 200)
IC.mnar.cc <- matrix(ncol = 2, nrow = 200)

IC.mcar.im <- matrix(ncol = 2, nrow = 200)
IC.mar.im <- matrix(ncol = 2, nrow = 200)
IC.mnar.im <- matrix(ncol = 2, nrow = 200)

IC.mcar.irs <- matrix(ncol = 2, nrow = 200)
IC.mar.irs <- matrix(ncol = 2, nrow = 200)
IC.mnar.irs <- matrix(ncol = 2, nrow = 200)

for (i in 1:200){
  don <- rmvnorm(n=n, mean = mu, sigma = matcov)
  colnames(don) <- c("X", "Y") 
  
  #MCAR
  don.ismcar = don
  ismcar <- sample(c(T,F), size = n, prob = c(0.35, 0.65), replace = TRUE)
  don.ismcar[ismcar, "Y"] <- NA
  don.ismcar <- as.data.frame(don.ismcar)
  
  #MAR
  don.ismar <- don
  ismar <- sapply(don.ismar[,"X"], FUN=function(xx){
    prob <- pnorm(1.2*xx-.5) 
    res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
    return (res)
  })
  don.ismar[ismar,"Y"] <- NA 
  don.ismar <- as.data.frame(don.ismar)
  
  #MNAR
  don.ismnar <- don
  ismnar <- sapply(don.ismnar[,"Y"], FUN=function(xx){
    prob <- pnorm(1.2*xx-.5) 
    res <- sample(c(T,F), size = 1, prob = c(prob,1-prob))
    return (res)
  })
  don.ismnar[ismnar,"Y"] <- NA 
  don.ismnar <- as.data.frame(don.ismnar)
  
  #Cas complet
  don.ismcar.cc <- na.omit(don.ismcar)
  don.ismar.cc <- na.omit(don.ismar)
  don.ismnar.cc <- na.omit(don.ismnar)
  
  lm.mcar.cc <- lm(don.ismcar.cc$Y~1)
  lm.mar.cc <- lm(don.ismar.cc$Y~1)
  lm.mnar.cc <- lm(don.ismnar.cc$Y~1)

  mean.mcar.cc[i] <- lm.mcar.cc$coefficients
  mean.mar.cc[i] <- lm.mar.cc$coefficients
  mean.mnar.cc[i] <- lm.mnar.cc$coefficients
  
  IC.mcar.cc[i,] <- confint(lm.mcar.cc)
  IC.mar.cc[i,] <- confint(lm.mar.cc)
  IC.mnar.cc[i,] <- confint(lm.mnar.cc)
  
  #Imputation par la moyenne
  don.ismcar.im <- na.aggregate(don.ismcar, FUN = mean)
  don.ismar.im <- na.aggregate(don.ismar, FUN = mean)
  don.ismnar.im <- na.aggregate(don.ismnar, FUN = mean)

  lm.mcar.im <- lm(don.ismcar.im$Y~1)
  lm.mar.im <- lm(don.ismar.im$Y~1)
  lm.mnar.im <- lm(don.ismnar.im$Y~1)

  mean.mcar.im[i] <- lm.mcar.im$coefficients
  mean.mar.im[i] <- lm.mar.im$coefficients
  mean.mnar.im[i] <- lm.mnar.im$coefficients
  
  IC.mcar.im[i,] <- confint(lm.mcar.im)
  IC.mar.im[i,] <- confint(lm.mar.im)
  IC.mnar.im[i,] <- confint(lm.mnar.im)
  
  #Imputation par régression stochastique
  don.ismcar.irs <- complete(mice(don.ismcar, method = "norm.nob", m = 1, print = FALSE))
  don.ismar.irs <- complete(mice(don.ismar, method = "norm.nob", m = 1, print = FALSE))
  don.ismnar.irs <- complete(mice(don.ismnar, method = "norm.nob", m = 1, print = FALSE))
  
  lm.mcar.irs <- lm(don.ismcar.irs$Y~1)
  lm.mar.irs <- lm(don.ismar.irs$Y~1)
  lm.mnar.irs <- lm(don.ismnar.irs$Y~1)

  mean.mcar.irs[i] <- lm.mcar.irs$coefficients
  mean.mar.irs[i] <- lm.mar.irs$coefficients
  mean.mnar.irs[i] <- lm.mnar.irs$coefficients
  
  IC.mcar.irs[i,] <- confint(lm.mcar.irs)
  IC.mar.irs[i,] <- confint(lm.mar.irs)
  IC.mnar.irs[i,] <- confint(lm.mnar.irs)
}
```

```{r, echo = F}
par(mfrow=c(3,1))
plot(mean.mcar.cc, type = 'l')
plot(mean.mar.cc, type = 'l')
plot(mean.mnar.cc, type = 'l')
```

```{r, echo = F}
mean.mcar.cc 
mean.mar.cc 
mean.mnar.cc 

mean.mcar.im
mean.mar.im 
mean.mnar.im  

mean.mcar.irs 
mean.mar.irs 
mean.mnar.irs

IC.mcar.cc 
IC.mar.cc 
IC.mnar.cc 

IC.mcar.im 
IC.mar.im 
IC.mnar.im 

IC.mcar.irs 
IC.mar.irs 
IC.mnar.irs 
```

## 2. Scénarios NA sur le jeu de données `mites`

```{r}
#install.packages("mice")
library("mice")
```

### **Titre**

Nous allons dans un premier temps générer 10% de données manquantes sur notre jeu de données `mites` suivant les 3 catégories utilisées precedement, MCAR, MAR et MNAR. Nous allons utiliser la fonction `ampute` du package `mice`.

-   Mécanisme **MCAR** (**M**issing **C**ompletely **A**t **R**andom)

```{r}
amp.mcar = ampute(mites, prop = 0.10, mech = "MCAR")
```

-   Mécanisme **MAR** (**M**issing **A**t **R**andom)

```{r}
amp.mar = ampute(mites, prop = 0.10, mech = "MAR")

```

-   Mécanisme **MNAR** (**M**issing **N**ot **A**t **R**andom)

```{r}
amp.mnar = ampute(mites, prop = 0.10, mech = "MNAR")
```

```{r}
kable(data.frame(c((amp.mnar$prop), (amp.mar$prop),(amp.mcar$prop)),row.names=c("MNAR", "MAR", "MCAR")), col.names=c("Proportion de données manquantes"))
```

On a bien 10% de données manquantes générées pour les 3 dispositifs.

### Analyse exploratoire

```{r}
#install.packages("funModeling")
#install.packages("VIM")
library("VIM")
library("funModeling")
```

Sites importants : <https://delladata.fr/decrire-description-descriptives/> <https://mran.microsoft.com/snapshot/2017-04-24/web/packages/mice/vignettes/ampute.html>

Explorer les 3 dispositifs de données manquantes créés. On pourra utiliser la fonction `aggr` du package `VIM` pour l'analyse univariée, `CramerV` du package `DescTools` pour l'analyse bivariée, `MCA` du package `FactoMineR` pour l'analyse multidimensionnelle

**Analyse univariée**

MCAR

```{r}
#install.packages("VIM")
library("VIM")
aggr(amp.mcar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=TRUE,
                  labels=names(amp.mcar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Histogram of missing data","Pattern"))

```

(Explication tirées du site delladata)

Les colonnes représentent les variables, les lignes représentent les types de situations dans le data frame. La ligne entière bleue représente toutes les lignes dans le data frame qui ne comportent pas de données manquantes (90% des lignes). Une case rouge représente toutes les lignes dans le data frame qui sont manquantes pour cette variable. Dans notre cas nous avons par exemple 2,9% pour la varible "Topo".

MAR

```{r}
aggr(amp.mar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=TRUE,
                  labels=names(amp.mar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Histogram of missing data","Pattern"))
```

Plusieurs cases rouges représentent les observations pour lesquelles les valeurs dans le data frame sont manquantes conjointement pour les variables concernées. Par exemple, 2,9% des lignes, ont une donnée manquante à la fois sur les variables "Substrate", "Shrub", "Topo" et "Prop". On observe qu'il n'y a pas de ligne qui ne comportent pas de données manquantes, en comparaison avec le dispositif MCAR. Autrement dit, chaque ligne contient des données manquantes.

MNAR

```{r}
aggr(amp.mnar$amp, col=c('navyblue','red'),
                  numbers=TRUE,
                  sortVars=TRUE,
                  labels=names(amp.mnar$amp),
                  cex.axis=.7, gap=3,
                  ylab=c("Histogram of missing data","Pattern"))
```

Ce dispositf MNAR se rapproche plus du dispositif MAR dans la description des données manquantes. En effet, chaque ligne contient aussi des données manquantes. On observe, de plus, que 100% des variables "Substrate", "Shrub" et "Topo" sont manquantes, comme dans le dispositif MAR.

**Analyse bivariée**

`CramerV` du package `DescTools` pour l'analyse bivariée

```{r}
library("DescTools")
?CramerV

tab <- table(mites$Substrate, mites$Shrub)
CramerV(tab)
```

\newpage

# Références bibliographiques
